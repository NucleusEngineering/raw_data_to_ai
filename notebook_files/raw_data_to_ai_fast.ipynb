{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upi2EY4L9ei3"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbF2F2miAT4a"
      },
      "source": [
        "# Data Done Right: üöÄ From Zero to Insights, Fast."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Learn how to build a scalable data stack that delivers actionable insights and powers AI-driven innovation. See it in action with a example use case that transforms raw data into business impact.\n",
        "\n",
        "This notebook demonstrates a simple but powerful use case example on Google Cloud. You'll learn how to:\n",
        "\n",
        "* **Get Data from an Environmental API**\n",
        "* **Create Synthetic Data with Gemini**\n",
        "* **Data Ingestion & Analysis with BigQuery**\n",
        "* **How to train a Machine Learning model with SQL**\n",
        "* **How to use LLMs in your Data Warehouse**"
      ],
      "metadata": {
        "id": "pXQyz_7JbPYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scenario\n",
        "\n",
        "### Building a Happiness Prediction App\n",
        "\n",
        "Imagine you're relocating.  One of the biggest questions is: how happy will I be?  This notebook tackles this challenge by building a happiness prediction app. Our core hypothesis is that air quality significantly impacts well-being, and therefore, happiness. We use real-world data on location, air quality, and happiness, supplementing it with synthetic data generated by a Large Language Model.  We then leverage the power of SQL within our data warehouse to train a machine learning model.  Finally, we demonstrate how to use this model to predict happiness scores for specific locations.\n",
        "\n",
        "*Diclaimer: the use case in this notebook was designed as inspiration. Some steps including the machine learning training on the given feature set might not represent a realistic scenario in the 'real world'.*"
      ],
      "metadata": {
        "id": "gGPOM8PWCaTA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbcZUjT1yvTq"
      },
      "source": [
        "## What you'll need\n",
        "\n",
        "* A Google Cloud Account and Google Cloud Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy9KqgPQ4GBi"
      },
      "source": [
        "## Basic Setup\n",
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_ppDxYf4Gqs"
      },
      "outputs": [],
      "source": [
        "%pip install \\\n",
        "  google-cloud-aiplatform \\\n",
        "    --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCiNGP1Qxd6x"
      },
      "source": [
        "### Connect Your Google Cloud Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLUGlG6UE2CK"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "project_id = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert project_id, \"‚ö†Ô∏è Please provide a Google Cloud project ID\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_DBzCfqHd3e"
      },
      "source": [
        "### Authenticate to Google Cloud within Colab\n",
        "If you're running this on google colab notebook, you will need to Authenticate as an IAM user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IAqJiZhHd3e"
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "\n",
        "credentials, project_id = google.auth.default()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Air Quality data for Google Form responses.\n",
        "\n",
        "In a previous step, we used Google Form to gather data from people about their home location and their happiness. The CSV file *FormResponses.csv* represents the CSV export from the Google Sheets that is connected to our Google Form.\n",
        "(Documentation on how to use Google Forms with Google Sheets [here](https://support.google.com/docs/answer/2917686?sjid=18278159077155361057-EU))"
      ],
      "metadata": {
        "id": "4klIu88vnLOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See how you can create your Environmental API key [here](https://developers.google.com/maps/documentation/javascript/get-api-key)."
      ],
      "metadata": {
        "id": "nXPdvo7fF-hA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1HkWTCb94bj"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the value below with your environmental API key and then run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "environmental_api_key = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert project_id, \"‚ö†Ô∏è Please provide a Google Cloud Environmental API Key\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Load the happiness survey data\n",
        "df = pd.read_csv('/content/FormResponses.csv') #The name of your CSV file\n",
        "\n",
        "# Extract location data & Happy Score form the CSV file\n",
        "latitudes = df['Your Address: Location Latitude (please use this format: 37.419734)'].values\n",
        "longitudes = df['Your Address: Location Longitude (please use this format: -122.0827784)'].values\n",
        "happy_score = df['How happy do you feel when spending time at home and in your neighborhood?'].values\n",
        "\n",
        "# AirQuality API endpoint and your API key\n",
        "api_endpoint = \"https://airquality.googleapis.com/v1/currentConditions:lookup\"\n",
        "api_key = environmental_api_key"
      ],
      "metadata": {
        "id": "c_a2c2eqzXAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_air_quality(latitude, longitude):\n",
        "  \"\"\"\n",
        "  Fetches air quality data for a given latitude and longitude using the AirQuality API.\n",
        "  Includes extended data (pollutants).\n",
        "  \"\"\"\n",
        "\n",
        "  params = {\n",
        "      \"key\": api_key\n",
        "  }\n",
        "  data = {\n",
        "      \"location\": {\n",
        "          \"latitude\": latitude,\n",
        "          \"longitude\": longitude\n",
        "      },\n",
        "      \"extraComputations\": [\n",
        "        \"HEALTH_RECOMMENDATIONS\",\n",
        "        \"DOMINANT_POLLUTANT_CONCENTRATION\",\n",
        "        \"POLLUTANT_CONCENTRATION\",\n",
        "        \"LOCAL_AQI\",\n",
        "        \"POLLUTANT_ADDITIONAL_INFO\"\n",
        "      ],\n",
        "      \"languageCode\": \"en\"\n",
        "  }\n",
        "  response = requests.post(api_endpoint, params=params, json=data)\n",
        "  response.raise_for_status()  # Raise an exception for bad status codes\n",
        "  data = response.json()\n",
        "  print (data)\n",
        "\n",
        "  # Access the pollutants data\n",
        "  pollutants = data['pollutants']\n",
        "\n",
        "  # Iterate through the pollutants and extract the values\n",
        "  for pollutant in pollutants:\n",
        "      code = pollutant['code']\n",
        "      display_name = pollutant['displayName']\n",
        "      full_name = pollutant['fullName']\n",
        "      value = pollutant['concentration']['value']\n",
        "      units = pollutant['concentration']['units']\n",
        "\n",
        "      #print(f\"{display_name} ({full_name}):\")\n",
        "      #print(f\"  Value: {value} {units}\")\n",
        "\n",
        "      # Instead of printing, add the data to a dictionary\n",
        "      pollutant_data = {\n",
        "          \"code\": code,\n",
        "          \"display_name\": display_name,\n",
        "          \"full_name\": full_name,\n",
        "          \"value\": value,\n",
        "          \"units\": units\n",
        "      }\n",
        "      data['pollutants'].append(pollutant_data)\n",
        "\n",
        "      return data"
      ],
      "metadata": {
        "id": "Pgz27JOQ-iSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch air quality data for each location from the CSV file.\n",
        "air_quality_data = []\n",
        "for lat, lon, h_score in zip(latitudes, longitudes, happy_score):\n",
        "    data = get_air_quality(lat, lon)\n",
        "    print(data)\n",
        "    if data:\n",
        "        pollutants_data = {}\n",
        "        for pollutant in data['pollutants']:\n",
        "            # Check if 'concentration' key exists before accessing it\n",
        "            if 'concentration' in pollutant:\n",
        "                pollutants_data[pollutant['code']] = pollutant['concentration']['value']\n",
        "        # Extract individual pollutants (if available) or use None if not found\n",
        "        pm25 = pollutants_data.get('pm25')\n",
        "        no2 = pollutants_data.get('no2')\n",
        "        o3 = pollutants_data.get('o3')\n",
        "        # ... extract other pollutants as needed ...\n",
        "        air_quality_data.append([lat, lon, pm25, no2, o3, h_score])  # Append data to the list\n",
        "    else:\n",
        "        # Handle the case where no data was returned for the location\n",
        "        air_quality_data.append([lat, lon, None, None, None, h_score])  # Append None values\n",
        "\n",
        "print(air_quality_data)"
      ],
      "metadata": {
        "id": "PbOEbwJz_Xdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Write the data to a new CSV file (without latitude and longitude)\n",
        "with open('air_quality_data.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['PM2.5', 'NO2', 'O3', 'happy_score'])  # Write header row (without lat/lon)\n",
        "    for row in air_quality_data:\n",
        "        writer.writerow(row[2:])  # Write data rows (excluding lat/lon)"
      ],
      "metadata": {
        "id": "0AWbC5KTANFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('air_quality_data.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for i in range(5):  # Read the first 5 rows\n",
        "        print(next(reader))"
      ],
      "metadata": {
        "id": "V9UhKH3YPiQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Synthetic Data with Gemini\n",
        "\n",
        "As we only were able to collect a few data points with our Google Form, we decide to create synthetic data that is based on our form responses. Doing so, we can ensure to have enough data points to train our machine learning model."
      ],
      "metadata": {
        "id": "GxJADgUMQzlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the JSON response schema that we expect from the Gemini output.\n",
        "response_schema = {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"PM2.5\": {\n",
        "                \"type\": \"number\",\n",
        "            },\n",
        "            \"NO2\": {\n",
        "                \"type\": \"number\",\n",
        "            },\n",
        "            \"O3\": {\n",
        "                \"type\": \"number\",\n",
        "            },\n",
        "            \"Happy_Score\": {\n",
        "                \"type\": \"integer\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [  # List of required properties\n",
        "            \"PM2.5\",\n",
        "            \"NO2\",\n",
        "            \"O3\",\n",
        "            \"Happy_Score\"\n",
        "        ],\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "xL8cKZyjJSK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More information about the model parameters of the foundation models can be found [here](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters)."
      ],
      "metadata": {
        "id": "7cmw7A3HIbOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
        "\n",
        "# Set up the model and generation configuration\n",
        "generation_config = vertexai.generative_models.GenerationConfig(\n",
        "    max_output_tokens=8192,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    response_mime_type=\"application/json\",\n",
        "    response_schema=response_schema\n",
        ")"
      ],
      "metadata": {
        "id": "ptMCbft1N1Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Read the CSV file\n",
        "with open('air_quality_data.csv', 'r', newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    data = list(reader)  # Read all rows into a list\n",
        "\n",
        "# Write the data to a TXT file to being able to send it to the Gemini API (CSV files were not supported when this notebook was created)\n",
        "with open('air_quality_data.txt', 'w') as txtfile:\n",
        "    for row in data:\n",
        "        txtfile.write('\\t'.join(row) + '\\n')  # Join values with tabs and add newline"
      ],
      "metadata": {
        "id": "RONgXD8EL8iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the Gemini API to generate synthetic data\n",
        "\n",
        "For more information and best practices regarinding prompting, check out the documentation [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design)."
      ],
      "metadata": {
        "id": "1hPfW6b3JTAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import json\n",
        "\n",
        "global json_string\n",
        "\n",
        "def generate():\n",
        "    vertexai.init(project=f\"{project_id}\", location=\"europe-west3\")\n",
        "    model = GenerativeModel(\n",
        "        \"gemini-1.5-flash-002\",\n",
        "        system_instruction=[\"\"\"You are a assistant that helps me create a synthetic dataset and output it in JSON format.\"\"\"]\n",
        "    )\n",
        "    responses = model.generate_content(\n",
        "        [\"\"\"Task: Generate Synthetic Dataset for Linear Regression Source Data:\"\"\", document1, text1],\n",
        "        generation_config=generation_config,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    json_string = \"\"\n",
        "    for response in responses:\n",
        "       print(response.text, end=\"\")\n",
        "       json_string += response.text\n",
        "\n",
        "    # Attempt to parse the accumulated string as JSON\n",
        "    try:\n",
        "        json_object = json.loads(json_string)\n",
        "        return json.dumps(json_object)  # Return the re-serialized JSON string\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        # Handle the error appropriately, e.g., return an error message or None\n",
        "        return None\n",
        "\n",
        "# Read the TXT file\n",
        "txt_file_path = \"/content/air_quality_data.txt\"\n",
        "\n",
        "with open(txt_file_path, \"r\") as f:\n",
        "    txt_data = f.read()\n",
        "\n",
        "encoded_data = base64.b64encode(txt_data.encode(\"utf-8\")).decode(\"utf-8\")\n",
        "document1 = Part.from_data(\n",
        "    mime_type=\"text/plain\",\n",
        "    data=encoded_data\n",
        ")\n",
        "\n",
        "text1 = \"\"\"Dataset Requirements:\n",
        "1. Context:\n",
        "  -The attached txt. file includes the initial data points for the dataset.\n",
        "  -The dataset consists of air quality data for a specific location and the corresponding happy score from one person.\n",
        "2. Data Enrichment:\n",
        "  - Randomly generate additional, diverse data points to expand the dataset.\n",
        "  - Ensure these new data points align with the distribution and characteristics of the original data points.\n",
        "3. Feature Selection:\n",
        "  - Include only these original features in the final synthetic dataset:\n",
        "    - PM2.5\n",
        "    - NO2\n",
        "    - O3\n",
        "    - Happy_Score (from 1 as very low and 5 as very high)\n",
        "4. Data Types and Relationships:\n",
        "  - Ensure the relationships between the Air Quality features and the happy score are suitable for linear regression.\n",
        "5. Output:\n",
        "  - Generate a JSON file containing 50 data points.\"\"\""
      ],
      "metadata": {
        "id": "OzwQTb_eR25y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the functions\n",
        "json_string = generate()"
      ],
      "metadata": {
        "id": "e5q7pafLreXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def write_to_csv(json_string):\n",
        "    \"\"\"Converts a JSON string to a CSV file.\n",
        "\n",
        "    Args:\n",
        "      json_string: A JSON string containing the data.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # 1. Parse the outer JSON string\n",
        "        data = json.loads(json_string)\n",
        "\n",
        "        # 2. Extract column names from the first item in the list\n",
        "        column_names = list(data[0].keys())\n",
        "\n",
        "        # 3. Write the data to a CSV file\n",
        "        with open('synthetic_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "            csv_writer.writerow(column_names)  # Write header row\n",
        "\n",
        "            # Write data rows (iterate through the list of dictionaries)\n",
        "            for row in data:\n",
        "                csv_writer.writerow(row.values())\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Key not found - {e}\")"
      ],
      "metadata": {
        "id": "3_w7-fodFa7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_to_csv(json_string)"
      ],
      "metadata": {
        "id": "tlY02gjFKSti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previewing our newly generated CSV file consisting of the Air Quality metrics and the happiness score."
      ],
      "metadata": {
        "id": "wMwMJNutJinl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('synthetic_data.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for i in range(5):  # Read the first 5 rows\n",
        "        print(next(reader))"
      ],
      "metadata": {
        "id": "9OfZZoFSS0_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn8g7-wCyZU6"
      },
      "source": [
        "## Set up BigQuery as your Data Warehouse\n",
        "In the next step we will import our dataset to a newly created dataset in BigQuery.\n",
        "Please set the following variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q2lc-Po1mPv"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the both the Google Cloud region and name of your BigQuery dataset and table.\n",
        "\n",
        "# Please fill in these values.\n",
        "multi_region = \"eu\"  # @param {type:\"string\"}\n",
        "dataset_name = \"happy_air\"  # @param {type:\"string\"}\n",
        "table_name = \"synthetic_data\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXI1uUu3y8gc"
      },
      "outputs": [],
      "source": [
        "# Quick input validations.\n",
        "assert multi_region, \"‚ö†Ô∏è Please provide a Google Cloud multi-region\"\n",
        "assert dataset_name, \"‚ö†Ô∏è Please provide the name of your dataset\"\n",
        "assert table_name, \"‚ö†Ô∏è Please provide the name of your table\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BigQuery Client for your project\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "xfVuJDC5Ppc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T616pEOUygYQ"
      },
      "source": [
        "### Create the BigQuery Dataset\n",
        "If you have already created an BigQuery Dataset, you can skip these steps and skip to the `Ingest data to your BigQuery dataset` section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyZYX4Jo1vfh"
      },
      "source": [
        "> ‚ú® We do use the command line bq command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQYni0NlTLzC"
      },
      "outputs": [],
      "source": [
        "!bq mk --location=$multi_region --dataset $project_id:$dataset_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LkscYH5Vfp"
      },
      "source": [
        "Let's check if the dataset is existing. Just list all the datasets in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkqQSWoY5Kab"
      },
      "outputs": [],
      "source": [
        "for dataset in client.list_datasets():\n",
        "  print(dataset.dataset_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K86id-dcjcm"
      },
      "source": [
        "## Ingest your CSV file into BigQuery\n",
        "\n",
        "This function will ingest your csv file into your BigQuery dataset.\n",
        "For documentation on how to load data into BigQuery check out [this link](https://https://cloud.google.com/bigquery/docs/loading-data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYKVQzv2cjcm"
      },
      "outputs": [],
      "source": [
        "# TODO(developer): Set table_id to the ID of the table to create.\n",
        "table_id = f\"{project_id}.{dataset_name}.{table_name}\"\n",
        "\n",
        "# TODO(developer): Set the file path to your local CSV file\n",
        "file_path = \"/content/synthetic_data.csv\"\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True\n",
        ")\n",
        "\n",
        "with open(file_path, \"rb\") as source_file:\n",
        "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
        "\n",
        "job.result()\n",
        "  # Waits for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_yNN1MnJpTR"
      },
      "source": [
        "## Analyse your data with Data Canvas\n",
        "\n",
        "For this exercise, head over to BigQuery Studio in your Google Cloud Console ([Link to BigQuery Studio](https://console.cloud.google.com/bigquery))."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   In the *BigQuery Explorer* navigate to your dataset.\n",
        "2.   Click on the toggle button for your dataset to see your table.\n",
        "3.   Click on the three dots for your table and select 'Query in Data Canvas'\n",
        "4.   A new tab with *Data Canvas* will open.\n",
        "5.   Begin to *'Query'*  your data with natural language prompts.\n",
        "\n",
        "\n",
        "Example prompts:\n",
        "\n",
        "*  **Show me the top 10 happiest data points**\n",
        "*  **What is the average happiness score?**\n",
        "*  **What is the maximum PM2.5 value?**\n",
        "*  **Show me the rows where the happiness score is higher and the PM2.5 level is also higher in relation to the average values.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6OzkDB8wgHn7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdolCWyatZmG"
      },
      "source": [
        "## Train your Machine Learning model with SQL code\n",
        "####In the next steps we will train a linear regression model with the synthetic dataset we just ingested into our BigQuery table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oaDyunPUnkd"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the name of your linear regression model.\n",
        "\n",
        "# Please fill in these values.\n",
        "model_name = \"ml_model_happy\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ml_model_id = f\"{project_id}.{dataset_name}.{model_name}\""
      ],
      "metadata": {
        "id": "4-_BfuQsUcFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The SQL statements will be executed via Python Magics for BigQuery ([Link to documentation](https://cloud.google.com/python/docs/reference/bigquery/latest/magics) )"
      ],
      "metadata": {
        "id": "B4_08NL3IwEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following SQL query uses BigQuery ML to train a linear regression model ([Docs for BigQuery ML](https://cloud.google.com/bigquery/docs/bqml-introduction)). We select all the features from our initial dataset and include 'Happy_Score' at the end as the value to predict.\n",
        "\n",
        "**Disclaimer:** *Our example features for training the Happy Air model are not quite realistic as all the features (the metrics regarding the air quality) are highly correlated. In an ideal example you would have a more diverse feature set to train your model. This would allow the model to learn more complex relationships and potentially make more accurate predictions.*"
      ],
      "metadata": {
        "id": "qci52moMLJHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{ml_model_id}`\n",
        "OPTIONS\n",
        "  (model_type='linear_reg',\n",
        "  input_label_cols=['Happy_Score']) AS\n",
        "SELECT\n",
        "  PM2_5,\n",
        "  NO2,\n",
        "  O3,\n",
        "  Happy_Score\n",
        "FROM\n",
        "  `{table_id}`\n",
        "\"\"\"\n",
        "print (query)\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()"
      ],
      "metadata": {
        "id": "Qk2qmRoZSxmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When navigating to your dataset in the Google Cloud console you will find your freshly trained model. Clicking on it you will find information about training details and evaluation metrics. You can also use the ML.Evaluate function to get evaluation metrics ([Documentation here](https://cloud.google.com/bigquery/docs/evaluate-overview))"
      ],
      "metadata": {
        "id": "Xw1KYXEeNd3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ml_model_id)"
      ],
      "metadata": {
        "id": "oGf23A_dUzHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project $project_id\n",
        "SELECT * FROM ML.EVALUATE(MODEL `{hard-code your ml_model_id here}`);"
      ],
      "metadata": {
        "id": "e5qcPmrOOMWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having our prompt column in place we need to create a model within BigQuery to be able to utilize GenAI capabilities.\n",
        "\n",
        "Firstly, we need to create an external connection within BigQuery to connect remotely to Vertex AI models."
      ],
      "metadata": {
        "id": "CPPhpAKKOOhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a connection to Vertex AI and create a LLM model in your BigQuery dataset."
      ],
      "metadata": {
        "id": "KV3oI0J0Vbin"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXgKKl_LQk5E"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the name of your connection to Vertex AI.\n",
        "\n",
        "# Please fill in these values.\n",
        "connection_name = \"connection_vertex\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bq mk --connection --location=$multi_region --project_id=$project_id \\\n",
        "    --connection_type=CLOUD_RESOURCE $connection_name"
      ],
      "metadata": {
        "id": "ogfNNbGMOTa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the connection to Vertex AI (and before importing the Gemini model into your BigQuery dataset) we have to grant the service account associated with the new connnection the relevant permissions to use Vertex AI:\n",
        "\n",
        "\n",
        "1.   Go to the [BigQuery console](https://console.cloud.google.com/bigquery).\n",
        "2.   In the *Explorer* tab on the left of your screen navigate to your *Project_ID* and click on it to show all the associated resources.\n",
        "3.   Click on *External Connections* and select the connection we just created.\n",
        "4.   The *Connection Info* will be displayed. Copy the *Service account id*.\n",
        "5.   Navigate to [IAM in the Google Cloud Console](https://console.cloud.google.com/iam-admin/iam).\n",
        "6.   Click on *+ Grant Access* and paste the *Service account id* from the previous steps into the *New Principal* field.\n",
        "7.   Select *Vertex AI User* as the role and click on *Save*.\n",
        "8.   Restart the Colab or Workbench Runtime and reload the page.\n",
        "9.   Execute the *Basic Setup* section of this notebook to authenticate again.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I5VvKaXeVjYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Gemini 2.0 Flash into your BigQuery dataset using the created connection to Vertex AI.**"
      ],
      "metadata": {
        "id": "SlRBW9KRXUsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgdHfEfbVh6a"
      },
      "outputs": [],
      "source": [
        "# @markdown Please fill in the name of your connection to Vertex AI.\n",
        "\n",
        "# Please fill in these values.\n",
        "gemini_model_bq = \"gemini-2.0-flash-001\"  # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "connection_id=f\"projects/{project_id}/locations/{multi_region}/connections/{connection_name}\""
      ],
      "metadata": {
        "id": "y4k6NtbzVyTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{project_id}.{dataset_name}.gemini_model_bq`\n",
        "  REMOTE WITH CONNECTION `{connection_id}`\n",
        "  OPTIONS (ENDPOINT = '{gemini_model_bq}');\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "  query_job = client.query(query)\n",
        "  query_job.result()  # Wait for the query to complete\n",
        "\n",
        "  # Check for errors in the job configuration or execution\n",
        "  if query_job.errors:\n",
        "    for error in query_job.errors:\n",
        "      print(f\"Error creating remote model: {error}\")\n",
        "  else:\n",
        "    print(\"Remote model created successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "F3eJD33hVMy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r16wPmxOBn_r"
      },
      "source": [
        "## Get predictions from your trained model and transform the output with Gemini.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paste in your latitude and longitude values so we can feed it into our linear regression model.**"
      ],
      "metadata": {
        "id": "5_FnZfYRmOvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mPx-Ns_mRRd"
      },
      "outputs": [],
      "source": [
        "# @markdown ### **Location Data:**\n",
        "\n",
        "Latitude = 52.51534871316592 # @param {type:\"number\"}\n",
        "Longitude = 13.396667668241038 # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "# Combine features into a dictionary\n",
        "feature_names = [\n",
        "    \"PM2_5\",\n",
        "    \"NO2\",\n",
        "    \"O3\"\n",
        "]\n",
        "\n",
        "global feature_values\n",
        "\n",
        "# Fetch air quality data for the location\n",
        "new_data = get_air_quality(Latitude, Longitude)\n",
        "print(new_data)\n",
        "\n",
        "new_air_quality_data = []\n",
        "\n",
        "if new_data:\n",
        "    new_pollutants_data = {}\n",
        "    for pollutant in new_data['pollutants']:\n",
        "        # Check if 'concentration' key exists before accessing it\n",
        "        if 'concentration' in pollutant:\n",
        "            new_pollutants_data[pollutant['code']] = pollutant['concentration']['value']\n",
        "    # Extract individual pollutants (if available) or use None if not found\n",
        "    pm25 = new_pollutants_data.get('pm25')\n",
        "    no2 = new_pollutants_data.get('no2')\n",
        "    o3 = new_pollutants_data.get('o3')\n",
        "\n",
        "    feature_values = [pm25, no2, o3]\n",
        "\n",
        "features = {name: value for name, value in zip(feature_names, feature_values)}\n",
        "print(features)\n",
        "# Now you can use the 'features' dictionary in your BigQuery query"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step we use the BigQuery client again to store the output in a dataframe.\n",
        "The query consists of two parts:\n",
        "\n",
        "\n",
        "1.   The inner part of the query gets a prediction from the linear regression model. As input we're using the features you selected in the cell above.\n",
        "2.   The outer part of the query uses the predicted value to get an interpretation of the result from the Gemini API.\n",
        "\n"
      ],
      "metadata": {
        "id": "KRT2MPGnjVKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Please make sure to insert your model ids into the SQL query.*"
      ],
      "metadata": {
        "id": "nj87vdTCn8c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT * FROM ML.GENERATE_TEXT(\n",
        "MODEL `your_project_id.your_dataset.your_gemini_model`,\n",
        "(\n",
        "    SELECT CONCAT(\"You are an AI assistant helping users understand how happy a person gets at a location based on AirQuality Data. Given the following information:The happiness level from 1 (being low) to 5 (being high) is: \", predicted_Happy_score, \". The AirQuality variable at the location are: \",\n",
        "    \"{', '.join(f'{name}= {value}' for name, value in features.items())}. \",\n",
        "    \"Generate a response that explains the predicted Happy_Score in a clear and friendly manner. Include insights into how the AirQuality might have influenced the score. Offer a funny suggestion on top. Answer in 6 sentences. Output the key words in bold and use paragraphs.\")\n",
        "    AS prompt\n",
        "    FROM (\n",
        "    SELECT *\n",
        "    FROM ML.EXPLAIN_PREDICT(\n",
        "        MODEL `your_project_id.your_dataset.your_linearregression_model`,\n",
        "        (SELECT {', '.join(f'{value} AS {name.replace(\" \", \"_\")}' for name, value in features.items())}),\n",
        "        STRUCT(3 as top_k_features)\n",
        "    ))),\n",
        "    STRUCT(\n",
        "      0.5 AS temperature,  -- Controls randomness (0.0 - 2.0)\n",
        "      8192 AS max_output_tokens, -- Limits the generated text length\n",
        "      0.95 AS top_p  -- Controls diversity of generated text\n",
        "    )\n",
        ");\n",
        "\"\"\"\n",
        "print (query)\n",
        "df = client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "IMKoAhgylUwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your results"
      ],
      "metadata": {
        "id": "8n35RHpJhCWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from IPython.display import Markdown\n",
        "\n",
        "generated_output = df [\"ml_generate_text_result\"][0]\n",
        "\n",
        "# Parse the JSON string\n",
        "data = json.loads(generated_output)\n",
        "\n",
        "# Extract the text\n",
        "text = data['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "display(Markdown(text))"
      ],
      "metadata": {
        "id": "64alNRpxfxT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "raw_data_to_ai_fast.ipynb"
    },
    "kernelspec": {
      "display_name": "python-docs-samples",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}